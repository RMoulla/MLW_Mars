{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RMoulla/MLW_Mars/blob/main/MLOPS_TP_BERT_Correction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMyf6lwuHII0"
      },
      "source": [
        "# **Modèle BERT pour l'analyse de sentiments**\n",
        "\n",
        "# Objectif du TP\n",
        "\n",
        "L’objectif de ce TP est de réaliser un fine-tuning d’un modèle BERT pré-entraîné pour la classification de commentaires sur des critiques de films (dataset IMDB). Concrètement, nous allons ajouter une couche linéaire au modèle BERT pré-entraîné, dont nous allons ajuster les poids sur un ensemble des données IMDB.\n",
        "\n",
        "Le TP se décline selon les étapes suivantes :\n",
        "\n",
        "# Étapes principales du TP\n",
        "\n",
        "1. **Charger un sous-ensemble du dataset IMDB** et séparer les données en jeu d’entraînement et jeu de test.\n",
        "2. **Convertir les critiques textuelles en séquences de tokens** grâce au tokenizer de BERT.\n",
        "3. **Créer une classe `IMDBDataset`** qui encapsule les textes, les labels et la logique d’encodage (tokenisation et gestion des masques).\n",
        "4. **Définir un modèle `BertClassifier`** composé d’un BERT pré-entraîné (gelé dans ce code) et d’une couche linéaire assurant la classification binaire.\n",
        "5. **Mettre en place la procédure d’entraînement** : pour plusieurs époques successives, parcourir les données d’entraînement par batch, calculer la perte et mettre à jour les paramètres de la couche de classification.\n",
        "6. **Évaluer périodiquement les performances** sur le jeu de test (calcul de la perte moyenne et de l’accuracy)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install torchvision --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nulymtmyj0f5",
        "outputId": "fc527dba-e631-494f-f3bc-16a426a8f6e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu118)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.8.86)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KNteB4Xp_0o",
        "outputId": "e675bff9-3ea0-4cd3-8f1c-0e7331de1bee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch: 2.6.0+cu118\n",
            "CUDA: 11.8\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "print(\"PyTorch:\", torch.__version__)\n",
        "print(\"CUDA:\", torch.version.cuda)\n",
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlaE16JVrqIK",
        "outputId": "1b1d8b39-c1d3-4b48-c489-b84faea9f3e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Mar 21 12:13:27 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   66C    P0             28W /   70W |     724MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPfV_cdYLGoB"
      },
      "source": [
        "\n",
        "## Préparation des données\n",
        "\n",
        "Nous définissons ici la classe IMDBDataset, héritée de Dataset. Elle prend en entrée des listes de textes et de labels binaires, ainsi qu’un tokenizer BERT et une taille maximale de séquence. Dans le constructeur, on vérifie que tous les labels sont bien à 0 ou 1, puis on enregistre le tout comme attributs internes. La méthode `__getitem__` réalise la tokenisation de chaque texte, avec un éventuel tronquage ou du padding pour atteindre la longueur requise. Elle renvoie finalement un dictionnaire contenant les tenseurs `input_ids`, `attention_mask` et `labels`, prêts à être utilisés par un modèle BERT lors de l’entraînement ou de l’inférence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQ8fmOck9GsL"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
        "        self.texts = texts # les commentaires\n",
        "        self.labels = labels  # labels 0 ou 1\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "        # Vérification des labels\n",
        "        unique_labels = set(self.labels)\n",
        "        print(f\"Labels uniques dans le dataset: {unique_labels}\")\n",
        "        # Comme c'est de la classification binaire, on veut {0, 1}\n",
        "        assert all(lbl in [0, 1] for lbl in unique_labels), \\\n",
        "            f\"Labels hors limite détectés: {unique_labels}\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Kc5bk4J96df",
        "outputId": "3dc00668-de58-47ca-c9c7-6331ef67f846"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chargement du dataset IMDB...\n"
          ]
        }
      ],
      "source": [
        "print(\"Chargement du dataset IMDB...\")\n",
        "dataset = load_dataset(\"imdb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpJAdVo21qZo",
        "outputId": "adf6ea60-8f0b-48cf-e654-f52376a86985"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taille du train : 25000\n",
            "Taille du test  : 25000\n"
          ]
        }
      ],
      "source": [
        "# Afficher la taille des splits\n",
        "print(f\"Taille du train : {len(dataset['train'])}\")\n",
        "print(f\"Taille du test  : {len(dataset['test'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWoQx9vu-Utf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "72d8ebe1-35fd-4b4b-adcb-2ca563431b55"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn\\'t matter what one\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\'t true. I\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\'re treated to the site of Vincent Gallo\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\'s bodies.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "dataset['train']['text'][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpdtgJRN-zaj",
        "outputId": "7568a557-196e-4c6f-af88-598288375ede"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "dataset['train']['label'][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSoCrXWwAkB7",
        "outputId": "e7a8a13f-7be9-417b-b697-b5eb39266d2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels uniques dans le dataset: {0}\n",
            "Texte original :\n",
            "\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn't matter what one's political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn't true. I've seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don't exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we're treated to the site of Vincent Gallo's throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won't see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women's bodies.\n",
            "\n",
            "Tokenized input :\n",
            "['\"', 'i', 'am', 'curious', ':', 'yellow', '\"', 'is', 'a', 'ri', '##sible', 'and', 'pre', '##ten', '##tious', 'steaming', 'pile', '.', 'it', 'doesn', \"'\", 't', 'matter', 'what', 'one', \"'\", 's', 'political', 'views', 'are', 'because', 'this', 'film', 'can', 'hardly', 'be', 'taken', 'seriously', 'on', 'any', 'level', '.', 'as', 'for', 'the', 'claim', 'that', 'frontal', 'male', 'nu', '##dity', 'is', 'an', 'automatic', 'nc', '-', '17', ',', 'that', 'isn', \"'\", 't', 'true', '.', 'i', \"'\", 've', 'seen', 'r', '-', 'rated', 'films', 'with', 'male', 'nu', '##dity', '.', 'granted', ',', 'they', 'only', 'offer', 'some', 'fleeting', 'views', ',', 'but', 'where', 'are', 'the', 'r', '-', 'rated', 'films', 'with', 'gaping', 'vu', '##lva', '##s', 'and', 'flap', '##ping', 'lab', '##ia', '?', 'nowhere', ',', 'because', 'they', 'don', \"'\", 't', 'exist', '.', 'the', 'same', 'goes', 'for', 'those', 'crap', '##py', 'cable', 'shows', ':', 'sc', '##hl', '##ong', '##s', 'swinging', 'in', 'the', 'breeze', 'but', 'not', 'a', 'clit', '##oris', 'in', 'sight', '.', 'and', 'those', 'pre', '##ten', '##tious', 'indie', 'movies', 'like', 'the', 'brown', 'bunny', ',', 'in', 'which', 'we', \"'\", 're', 'treated', 'to', 'the', 'site', 'of', 'vincent', 'gallo', \"'\", 's', 'throbbing', 'johnson', ',', 'but', 'not', 'a', 'trace', 'of', 'pink', 'visible', 'on', 'chloe', 'se', '##vi', '##gny', '.', 'before', 'crying', '(', 'or', 'implying', ')', '\"', 'double', '-', 'standard', '\"', 'in', 'matters', 'of', 'nu', '##dity', ',', 'the', 'mentally', 'ob', '##tus', '##e', 'should', 'take', 'into', 'account', 'one', 'una', '##vo', '##ida', '##bly', 'obvious', 'anatomical', 'difference', 'between', 'men', 'and', 'women', ':', 'there', 'are', 'no', 'gen', '##ital', '##s', 'on', 'display', 'when', 'actresses', 'appears', 'nude', ',', 'and', 'the', 'same', 'cannot', 'be', 'said', 'for', 'a', 'man', '.', 'in', 'fact', ',', 'you', 'generally', 'won', \"'\", 't', 'see', 'female', 'gen', '##ital', '##s', 'in', 'an', 'american', 'film', 'in', 'anything', 'short', 'of', 'porn', 'or', 'explicit', 'erotic', '##a', '.', 'this', 'alleged', 'double', '-', 'standard', 'is', 'less', 'a', 'double', 'standard', 'than', 'an', 'admitted', '##ly', 'de', '##pressing', 'ability', 'to', 'come', 'to', 'terms', 'culturally', 'with', 'the', 'insides', 'of', 'women', \"'\", 's', 'bodies', '.']\n",
            "\n",
            "Encodage final (input_ids) :\n",
            "tensor([  101,  1045, 12524,  1045,  2572,  8025,  1011,  3756,  2013,  2026,\n",
            "         2678,  3573,  2138,  1997,  2035,  1996,  6704,  2008,  5129,  2009,\n",
            "         2043,  2009,  2001,  2034,  2207,  1999,  3476,  1012,  1045,  2036,\n",
            "         2657,  2008,  2012,  2034,  2009,  2001,  8243,  2011,  1057,  1012,\n",
            "         1055,  1012,  8205,  2065,  2009,  2412,  2699,  2000,  4607,  2023,\n",
            "         2406,  1010,  3568,  2108,  1037,  5470,  1997,  3152,  2641,  1000,\n",
            "         6801,  1000,  1045,  2428,  2018,  2000,  2156,  2023,  2005,  2870,\n",
            "         1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  1996,\n",
            "         5436,  2003,  8857,  2105,  1037,  2402,  4467,  3689,  3076,  2315,\n",
            "        14229,  2040,  4122,  2000,  4553,  2673,  2016,  2064,  2055,  2166,\n",
            "         1012,  1999,  3327,  2016,  4122,  2000,  3579,  2014,  3086,  2015,\n",
            "         2000,  2437,  2070,  4066,  1997,  4516,  2006,  2054,  1996,  2779,\n",
            "        25430, 14728,  2245,  2055,  3056,  2576,  3314,  2107,  2004,  1996,\n",
            "         5148,  2162,  1998,  2679,  3314,  1999,  1996,  2142,  2163,  1012,\n",
            "         1999,  2090,  4851,  8801,  1998,  6623,  7939,  4697,  3619,  1997,\n",
            "         8947,  2055,  2037, 10740,  2006,  4331,  1010,  2016,  2038,  3348,\n",
            "         2007,  2014,  3689,  3836,  1010, 19846,  1010,  1998,  2496,  2273,\n",
            "         1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  2054,\n",
            "         8563,  2033,  2055,  1045,  2572,  8025,  1011,  3756,  2003,  2008,\n",
            "         2871,  2086,  3283,  1010,  2023,  2001,  2641, 26932,  1012,  2428,\n",
            "         1010,  1996,  3348,  1998, 16371, 25469,  5019,  2024,  2261,  1998,\n",
            "         2521,  2090,  1010,  2130,  2059,  2009,  1005,  1055,  2025,  2915,\n",
            "         2066,  2070, 10036,  2135,  2081, 22555,  2080,  1012,  2096,  2026,\n",
            "         2406,  3549,  2568,  2424,  2009, 16880,  1010,  1999,  4507,  3348,\n",
            "         1998, 16371, 25469,  2024,  1037,  2350, 18785,  1999,  4467,  5988,\n",
            "         1012,  2130, 13749,  7849, 24544,  1010, 15835,  2037,  3437,  2000,\n",
            "         2204,  2214,  2879,  2198,  4811,  1010,  2018,  3348,  5019,  1999,\n",
            "         2010,  3152,  1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,\n",
            "         1028,  1045,  2079,  4012,  3549,  2094,  1996, 16587,  2005,  1996,\n",
            "         2755,  2008,  2151,  3348,  3491,  1999,  1996,  2143,  2003,  3491,\n",
            "         2005,  6018,  5682,  2738,  2084,  2074,  2000,  5213,  2111,  1998,\n",
            "         2191,  2769,  2000,  2022,  3491,  1999, 26932, 12370,  1999,  2637,\n",
            "         1012,  1045,  2572,  8025,  1011,  3756,  2003,  1037,  2204,  2143,\n",
            "         2005,  3087,  5782,  2000,  2817,  1996,  6240,  1998, 14629,  1006,\n",
            "         2053, 26136,  3832,  1007,  1997,  4467,  5988,  1012,  2021,  2428,\n",
            "         1010,  2023,  2143,  2987,  1005,  1056,  2031,  2172,  1997,  1037,\n",
            "         5436,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0])\n",
            "\n",
            "Attention mask :\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n",
            "\n",
            "Label associé :\n",
            "tensor(0)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "sample = IMDBDataset(dataset['train']['text'][:1], dataset['train']['label'][:1], tokenizer, max_len=512)[0]\n",
        "\n",
        "print(\"Texte original :\")\n",
        "print(dataset['train']['text'][1])\n",
        "print(\"\\nTokenized input :\")\n",
        "print(tokenizer.tokenize(dataset['train']['text'][1]))\n",
        "\n",
        "print(\"\\nEncodage final (input_ids) :\")\n",
        "print(sample[\"input_ids\"])  # IDs des tokens\n",
        "print(\"\\nAttention mask :\")\n",
        "print(sample[\"attention_mask\"])  # 1s et 0s\n",
        "print(\"\\nLabel associé :\")\n",
        "print(sample[\"labels\"])  # Label (0 ou 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnGkLwuuN0G5"
      },
      "source": [
        "## Construction et entraînement du modèle de classification\n",
        "\n",
        "Nous allons construire le modèle de classification de sentiments sur un sous-ensemble du dataset IMDB à l’aide d’un modèle `BertClassifier`. Il définit notamment deux fonctions d’entraînement et d’évaluation, configure les hyperparamètres, crée les jeux de données et exécute plusieurs époques d’apprentissage pour optimiser la couche de classification.\n",
        "\n",
        "- **Définition du modèle** : La classe `BertClassifier` combine un modèle BERT pré-entraîné et une couche linéaire pour la classification.\n",
        "\n",
        "- **Fonction d’entraînement (`train_epoch`)** : Elle parcourt les données par batch, calcule la perte (cross-entropy), met à jour la couche de classification et retourne la perte moyenne et l’accuracy.\n",
        "\n",
        "- **Fonction d’évaluation (`eval_model`)** : Elle parcourt les données de validation ou de test, calcule la perte moyenne, l’accuracy et retourne ces métriques sans modifier le modèle.\n",
        "\n",
        "- **Configuration des hyperparamètres** : On définit ici les hyperparamètres du modèle :batch size, taux d’apprentissage, nombre d’époques, taille maximale des séquences et  nombre de classes.\n",
        "\n",
        "- **Chargement du dataset IMDB** : On sélectionne les échantillons d'entraînement et de test.\n",
        "\n",
        "- **Tokenisation** : On utilise `BertTokenizer` pour transformer les critiques textuelles en séquences de tokens.\n",
        "\n",
        "- **Création des `IMDBDataset`** : On convertit des tokens et labels en objets PyTorch (input_ids, attention_mask, labels).\n",
        "\n",
        "- **Initialisation du modèle** : Instanciation de `BertClassifier` et association au device (CPU ou GPU).\n",
        "\n",
        "- **Boucle d’entraînement** : Pour chaque époque, on entraîne le modèle sur les données d’entraînement puis on l’évalue sur les données de validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eq5BKZ5MpQeo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import BertModel, BertTokenizer, AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# Vérifier si un GPU est disponible\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Utilisation de : {device}\")\n",
        "\n",
        "######## Définition du modèle ##########\n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(BertClassifier, self).__init__()\n",
        "        # On charge BERT (base uncased)\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.drop = nn.Dropout(p=0.3)\n",
        "\n",
        "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Retour par défaut : dictionnaire (return_dict=True)\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        # Récupération du \"pooler_output\"\n",
        "        pooled_output = outputs.pooler_output  # On récupère le token [CLS]\n",
        "        output = self.drop(pooled_output)\n",
        "        return self.out(output)\n",
        "\n",
        "\n",
        "#### Définition de la fonction train ####\n",
        "\n",
        "\n",
        "def train_epoch(model, data_loader, optimizer, device):\n",
        "    # Geler les paramètres de BERT\n",
        "    for param in model.bert.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for batch in tqdm(data_loader, desc=\"Training\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        correct_predictions += torch.sum(preds == labels)\n",
        "        total_predictions += labels.size(0)\n",
        "\n",
        "    return total_loss / len(data_loader), correct_predictions.double() / total_predictions\n",
        "\n",
        "\n",
        "#### Définition de la fonction eval ####\n",
        "\n",
        "def eval_model(model, data_loader, device):\n",
        "    \"\"\"\n",
        "    Évalue les performances du modèle sur un jeu de validation ou de test.\n",
        "\n",
        "    :param model: Modèle BERT ou GPT fine-tuné.\n",
        "    :param data_loader: DataLoader contenant les données à évaluer.\n",
        "    :param device: Appareil à utiliser (CPU ou GPU).\n",
        "    :return: Moyenne de la perte et précision sur les données évaluées.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    # Désactiver la dérivation automatique\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Calcul des prédictions\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "            # Calcul de la perte\n",
        "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Calcul des prédictions correctes\n",
        "            correct_predictions += torch.sum(preds == labels)\n",
        "            total_predictions += labels.size(0)\n",
        "\n",
        "    # Moyenne des pertes et précision totale\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    accuracy = correct_predictions.double() / total_predictions\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "\n",
        "######## Entraînement du modèle #########\n",
        "\n",
        "\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 5\n",
        "MAX_LEN = 128\n",
        "LEARNING_RATE = 2e-4\n",
        "N_CLASSES = 2  # Binaire : positif (1) ou négatif (0)\n",
        "\n",
        "\n",
        "# On prend seulement une partie du dataset pour ce test\n",
        "# - Ex : 10000 pour l'entraînement\n",
        "# - Ex : 500 pour le test\n",
        "train_texts = dataset['train']['text']\n",
        "train_labels = dataset['train']['label']\n",
        "test_texts = dataset['test']['text']\n",
        "test_labels = dataset['test']['label']\n",
        "\n",
        "print(f\"Taille du dataset d'entraînement : {len(train_texts)}\")\n",
        "print(f\"Taille du dataset de test : {len(test_texts)}\")\n",
        "\n",
        "print(\"Distribution des labels (train):\", np.bincount(train_labels))\n",
        "print(\"Distribution des labels (test) :\", np.bincount(test_labels))\n",
        "\n",
        "print(\"Initialisation du tokenizer...\")\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "print(\"Création des datasets...\")\n",
        "train_dataset = IMDBDataset(train_texts, train_labels, tokenizer, max_len=MAX_LEN)\n",
        "val_dataset = IMDBDataset(test_texts, test_labels, tokenizer, max_len=MAX_LEN)\n",
        "\n",
        "print(\"Création des dataloaders...\")\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "print(\"Initialisation du modèle...\")\n",
        "model = BertClassifier(N_CLASSES).to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "best_accuracy = 0\n",
        "\n",
        "print(\"Début de l'entraînement...\")\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f'\\nEpoch {epoch + 1}/{EPOCHS}')\n",
        "\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, device)\n",
        "    print(f'Train loss: {train_loss:.3f}, accuracy: {train_acc:.3f}')\n",
        "\n",
        "    val_loss, val_acc = eval_model(model, val_loader, device)\n",
        "    print(f'Val loss: {val_loss:.3f}, accuracy: {val_acc:.3f}')\n",
        "\n",
        "    # Sauvegarde du meilleur modèle si l'accuracy s'améliore\n",
        "    if val_acc > best_accuracy:\n",
        "        best_accuracy = val_acc\n",
        "        torch.save(model.state_dict(), 'best_model_imdb.pt')\n",
        "        print(f'Nouveau meilleur modèle sauvegardé avec accuracy: {val_acc:.3f}')\n",
        "\n",
        "    print('-' * 50)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNPhsrrpOLRajmfOLlm6o4j",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}